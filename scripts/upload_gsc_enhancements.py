#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================
# File: upload_gsc_enhancements.py
# Revision: Rev.13 â€” Fixed all column & schema issues
# ============================================================

import os
import pandas as pd
from google.cloud import bigquery
from google.cloud.bigquery import SchemaField

# ===============================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ
# ===============================
PROJECT_ID = "bamtabridsazan"
DATASET_ID = "seo_reports"
TABLE_ID = "bamtabridsazan__gsc__raw_enhancements"
GITHUB_LOCAL_PATH = "gsc_enhancements"
UNIQUE_KEY_COLUMNS = ["url", "item_name", "last_crawled", "enhancement_type"]

client = bigquery.Client(project=PROJECT_ID)

# ===============================
# ØªØ§Ø¨Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ (ÙˆÛŒØ±Ø§ÛŒØ´ Û±Û°)
# ===============================
def parse_excel_file(file_path, site, appearance_type):
    """
    Parse Excel file and extract normalized data for BigQuery.
    - Handles case-insensitive sheet names (Chart / Table)
    - Normalizes column names (lowercase, underscores)
    - Ensures all required columns exist
    """

    try:
        # ğŸ”¹ Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ùˆ Ù„ÛŒØ³Øª Ø´ÛŒØªâ€ŒÙ‡Ø§
        wb = load_workbook(file_path, data_only=True)
        sheet_names = [s.lower() for s in wb.sheetnames]

        # ğŸ”¹ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ Ø´ÛŒØªâ€ŒÙ‡Ø§ (case-insensitive)
        chart_sheet_name = None
        table_sheet_name = None
        for s in wb.sheetnames:
            sl = s.lower()
            if "chart" in sl:
                chart_sheet_name = s
            elif "table" in sl:
                table_sheet_name = s

        if not chart_sheet_name and not table_sheet_name:
            raise ValueError("No Chart or Table sheet found in file.")

        records = []

        # ğŸ”¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÛŒØª Chart (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
        if chart_sheet_name:
            df_chart = pd.read_excel(file_path, sheet_name=chart_sheet_name)

            # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
            df_chart.columns = [str(c).strip().lower().replace(" ", "_") for c in df_chart.columns]

            # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ† date
            if "date" not in df_chart.columns:
                raise ValueError(f"'date' column not found in Chart sheet: {chart_sheet_name}")

            # ÙÙ‚Ø· Ø³ØªÙˆÙ† ØªØ§Ø±ÛŒØ® Ø±Ùˆ Ù†Ú¯Ù‡ Ù…ÛŒâ€ŒØ¯Ø§Ø±ÛŒÙ…
            df_chart = df_chart[["date"]].dropna(subset=["date"])
            df_chart["date"] = pd.to_datetime(df_chart["date"], errors="coerce").dt.date

            chart_dates = df_chart["date"].dropna().unique().tolist()
        else:
            chart_dates = []

        # ğŸ”¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÛŒØª Table (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
        if table_sheet_name:
            df_table = pd.read_excel(file_path, sheet_name=table_sheet_name)
            df_table.columns = [str(c).strip().lower().replace(" ", "_") for c in df_table.columns]

            # ØªØ¹Ø±ÛŒÙ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯Ù†ÛŒØ§Ø²
            required_cols = ["url", "item_name", "last_crawled", "status"]
            for col in required_cols:
                if col not in df_table.columns:
                    df_table[col] = None

            df_table["last_crawled"] = pd.to_datetime(df_table["last_crawled"], errors="coerce").dt.date

            # Ø§Ú¯Ø± Ø´ÛŒØª Chart ØªØ§Ø±ÛŒØ® Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ù…Ù‡ Ø³Ø·Ø±Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù† ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ ØªÚ©Ø«ÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            if chart_dates:
                for d in chart_dates:
                    for _, row in df_table.iterrows():
                        records.append({
                            "date": d,
                            "url": row.get("url"),
                            "item_name": row.get("item_name"),
                            "last_crawled": row.get("last_crawled"),
                            "site": site,
                            "appearance_type": appearance_type,
                            "status": row.get("status"),
                        })
            else:
                # Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯Ù† chart_datesØŒ ÙÙ‚Ø· Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ table Ø±Ø§ Ø¨Ø¯ÙˆÙ† ØªØ§Ø±ÛŒØ® Ù…ÛŒâ€ŒÙØ±Ø³ØªÛŒÙ…
                for _, row in df_table.iterrows():
                    records.append({
                        "date": None,
                        "url": row.get("url"),
                        "item_name": row.get("item_name"),
                        "last_crawled": row.get("last_crawled"),
                        "site": site,
                        "appearance_type": appearance_type,
                        "status": row.get("status"),
                    })

        # ğŸ”¹ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ DataFrame Ù†Ù‡Ø§ÛŒÛŒ
        if not records:
            raise ValueError("No records extracted from file.")

        df = pd.DataFrame(records)

        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        df.columns = [str(c).strip().lower().replace(" ", "_") for c in df.columns]

        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù‡Ù…Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©ÛŒÙ…Ø§ BQ
        all_cols = ["date", "url", "item_name", "last_crawled", "site", "appearance_type", "status", "unique_key"]
        for col in all_cols:
            if col not in df.columns:
                df[col] = None

        # Ø³Ø§Ø®Øª unique_key
        df["unique_key"] = df.apply(
            lambda r: f"{r['site']}_{r['appearance_type']}_{r['url']}_{r['item_name']}_{r['date']}", axis=1
        )

        return df

    except Exception as e:
        print(f"âŒ Error parsing {file_path}: {e}")
        return pd.DataFrame(columns=["date", "url", "item_name", "last_crawled", "site", "appearance_type", "status", "unique_key"])


# ===============================
# ØªØ§Ø¨Ø¹ Ø§ÛŒØ¬Ø§Ø¯ Unique Key
# ===============================
def create_unique_key(df, columns):
    df = df.copy()
    df["unique_key"] = df[columns].astype(str).agg("__".join, axis=1)
    return df

# ===============================
# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¬Ø¯ÙˆÙ„ Ø¯Ø± / Ø§ÛŒØ¬Ø§Ø¯ Ø¢Ù† BQ
# ===============================
def ensure_table_exists():
    table_ref = client.dataset(DATASET_ID).table(TABLE_ID)
    try:
        client.get_table(table_ref)
        print(f"âœ… Table {DATASET_ID}.{TABLE_ID} already exists.")
    except Exception:
        print(f"âš™ï¸ Table {DATASET_ID}.{TABLE_ID} not found. Creating...")
        schema = [
            SchemaField("date", "DATE"),
            SchemaField("url", "STRING"),
            SchemaField("item_name", "STRING"),
            SchemaField("last_crawled", "DATE"),
            SchemaField("site", "STRING"),
            SchemaField("appearance_type", "STRING"),
            SchemaField("status", "STRING"),
            SchemaField("unique_key", "STRING"),
        ]
        table = bigquery.Table(table_ref, schema=schema)
        client.create_table(table)
        print(f"âœ… Table {DATASET_ID}.{TABLE_ID} created successfully.")

# ===============================
# Ø¯Ø±ÛŒØ§ÙØª Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø² BQ
# ===============================
def get_existing_unique_keys():
    query = f"SELECT DISTINCT unique_key FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`"
    df = client.query(query).to_dataframe()
    return set(df["unique_key"].tolist())

# ===============================
# Ø¢Ù¾Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ BQ
# ===============================
def append_to_bigquery(df):
    # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ® Ø¨Ù‡ DATE
    for date_col in ["date", "last_crawled"]:
        if date_col in df.columns:
            df[date_col] = pd.to_datetime(df[date_col], errors="coerce").dt.date

    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡ Ù…Ø·Ø§Ø¨Ù‚ Ø§Ø³Ú©ÛŒÙ…Ø§
    schema_cols = ["date", "url", "item_name", "last_crawled", "site", "appearance_type", "status", "unique_key"]
    for col in schema_cols:
        if col not in df.columns:
            df[col] = None
    df = df[schema_cols]

    job_config = bigquery.LoadJobConfig(write_disposition="WRITE_APPEND")
    job = client.load_table_from_dataframe(df, f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}", job_config=job_config)
    job.result()
    print(f"âœ… Uploaded {len(df)} new rows")

# ===============================
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
# ===============================
ensure_table_exists()

VALID_COLUMNS = ["date", "url", "item_name", "last_crawled", "site", "appearance_type", "status", "unique_key"]
def main():
    enhancement_folder = "enhancements"
    enhancement_files = get_excel_files(enhancement_folder)

    all_new_records = []
    existing_keys = load_existing_unique_keys()

    for file_path in enhancement_files:
        site = "bamtabridsazan.com"   # Ø¯Ø§Ù…Ù†Ù‡ Ø³Ø§ÛŒØª
        appearance_type = enhancement_folder  # Ù†ÙˆØ¹ Enhancement

        df = parse_excel_file(file_path, site, appearance_type)
        if df is None or df.empty:
            continue

        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]

        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…
        for col in ["url", "item_name", "last_crawled"]:
            if col not in df.columns:
                df[col] = None

        # Ø³Ø§Ø®Øª unique key
        df = create_unique_key(df, ["url", "item_name", "last_crawled", "appearance_type"])

        # ÙÛŒÙ„ØªØ± Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
        new_df = df[~df["unique_key"].isin(existing_keys)]
        existing_keys.update(new_df["unique_key"].tolist())

        if not new_df.empty:
            all_new_records.append(new_df)

    # âœ… Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø±Ùˆ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ù…ÛŒÙ†â€ŒØ·ÙˆØ±ÛŒ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
    if all_new_records:
        final_df = pd.concat(all_new_records, ignore_index=True)
        append_to_bigquery(final_df)
    else:
        print("âš ï¸ No new records found.")


if __name__ == "__main__":
    main()
