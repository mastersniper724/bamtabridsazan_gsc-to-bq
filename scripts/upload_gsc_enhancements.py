
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ============================================================
# File: upload_gsc_enhancements.py
# Revision: Rev.7 â€” read existing_keys once; Duplicate-check enforced for Blocks B & C; logs improved
# Purpose: Full fetch from GSC -> BigQuery with duplicate prevention and sitewide total batch
# ============================================================

import os
import re
import pandas as pd
from google.cloud import bigquery
from google.cloud.bigquery import SchemaField

# ===============================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ
# ===============================
PROJECT_ID = "bamtabridsazan"
DATASET_ID = "seo_reports"
TABLE_ID = "bamtabridsazan__gsc__raw_enhancements"
GITHUB_LOCAL_PATH = "gsc_enhancements"  # Ù…Ø³ÛŒØ± Ø±ÛŒØ´Ù‡ ÙÙˆÙ„Ø¯Ø±
UNIQUE_KEY_COLUMNS = ["url", "item_name", "last_crawled", "enhancement_type"]

client = bigquery.Client(project=PROJECT_ID)

# ===============================
# ØªØ§Ø¨Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„
# ===============================
def parse_excel_file(file_path, enhancement_type):
    try:
        xls = pd.ExcelFile(file_path)
        sheets = xls.sheet_names

        chart_df = None
        table_df = None
        metadata_df = None

        if "Chart" in sheets:
            chart_df = pd.read_excel(xls, "Chart")
        if "Table sheet" in sheets:
            table_df = pd.read_excel(xls, "Table sheet")
        if "Metadata" in sheets:
            metadata_df = pd.read_excel(xls, "Metadata")

        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† Enhancement Type
        if chart_df is not None:
            chart_df["enhancement_type"] = enhancement_type
        if table_df is not None:
            table_df["enhancement_type"] = enhancement_type
        if metadata_df is not None:
            metadata_df["enhancement_type"] = enhancement_type

        return chart_df, table_df, metadata_df
    except Exception as e:
        print(f"âŒ Error parsing {file_path}: {e}")
        return None, None, None

# ===============================
# ØªØ§Ø¨Ø¹ Ø§ÛŒØ¬Ø§Ø¯ Unique Key
# ===============================
def create_unique_key(df, columns):
    df = df.copy()
    df["unique_key"] = df[columns].astype(str).agg("__".join, axis=1)
    return df

# ===============================
# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¬Ø¯ÙˆÙ„ Ø¯Ø± / Ø§ÛŒØ¬Ø§Ø¯ Ø¢Ù† BQ
# ===============================
def ensure_table_exists():
    """Create the target BigQuery table if it doesn't exist."""
    from google.cloud.bigquery import SchemaField

    dataset_id = "seo_reports"
    table_id = "bamtabridsazan__gsc__raw_enhancements"
    table_ref = client.dataset(dataset_id).table(table_id)

    try:
        client.get_table(table_ref)
        print(f"âœ… Table {dataset_id}.{table_id} already exists.")
    except Exception:
        print(f"âš™ï¸ Table {dataset_id}.{table_id} not found. Creating...")
        schema = [
            SchemaField("date", "DATE"),             # Ø³ØªÙˆÙ† ØªØ§Ø±ÛŒØ® Ø§Ø² Chart sheet
            SchemaField("url", "STRING"),            # Ø³ØªÙˆÙ† URL Ø§Ø² Table sheet
            SchemaField("item_name", "STRING"),      # Ø³ØªÙˆÙ† Item name Ø§Ø² Table sheet
            SchemaField("last_crawled", "DATE"),    # Ø³ØªÙˆÙ† Last crawled Ø§Ø² Table sheet
            SchemaField("site", "STRING"),           # Ø«Ø§Ø¨Øª ÛŒØ§ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù… Ø¯Ø§Ù…Ù†Ù‡
            SchemaField("appearance_type", "STRING"),# Breadcrumb / FAQ / Recipe Ùˆ ØºÛŒØ±Ù‡
            SchemaField("status", "STRING"),         # Valid / Invalid ÛŒØ§ Ù…Ø´Ø§Ø¨Ù‡
            SchemaField("unique_key", "STRING"),     # hash ÛŒØ§ ØªØ±Ú©ÛŒØ¨ ÛŒÚ©ØªØ§
        ]
        table = bigquery.Table(table_ref, schema=schema)
        client.create_table(table)
        print(f"âœ… Table {dataset_id}.{table_id} created successfully.")

# ===============================
# Ø¯Ø±ÛŒØ§ÙØª Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø² BQ
# ===============================
def get_existing_unique_keys():
    query = f"SELECT DISTINCT unique_key FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`"
    df = client.query(query).to_dataframe()
    return set(df["unique_key"].tolist())

# ===============================
# Ø¢Ù¾Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ BQ
# ===============================
def append_to_bigquery(df):
    # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ† date Ø¨Ù‡ Ù†ÙˆØ¹ DATE Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ BigQuery
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.date

    job_config = bigquery.LoadJobConfig(write_disposition="WRITE_APPEND")
    job = client.load_table_from_dataframe(df, f"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}", job_config=job_config)
    job.result()
    print(f"âœ… Uploaded {len(df)} new rows")

# ===============================
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
# ===============================
ensure_table_exists()

# Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ù…Ø·Ø§Ø¨Ù‚ Ø¬Ø¯ÙˆÙ„ BQ
VALID_COLUMNS = ["site", "appearance_type", "status", "unique_key", "url", "item_name", "last_crawled"]

def main():
    all_new_records = []
    existing_keys = get_existing_unique_keys()
    print(f"ğŸ”¹ Loaded {len(existing_keys)} existing unique keys")

    for enhancement_folder in os.listdir(GITHUB_LOCAL_PATH):
        folder_path = os.path.join(GITHUB_LOCAL_PATH, enhancement_folder)
        if not os.path.isdir(folder_path):
            continue

        for file_name in os.listdir(folder_path):
            if not file_name.endswith(".xlsx"):
                continue

            file_path = os.path.join(folder_path, file_name)
            enhancement_type = enhancement_folder  # Ù†Ø§Ù… ÙÙˆÙ„Ø¯Ø±

            print(f"ğŸ“„ Processing {file_path}")

            chart_df, table_df, metadata_df = parse_excel_file(file_path, enhancement_type)

            for df in [chart_df, table_df, metadata_df]:
                if df is None or df.empty:
                    continue

                # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
                df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]

                # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…
                for col in ["url", "item_name", "last_crawled"]:
                    if col not in df.columns:
                        df[col] = None

                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† enhancement_type
                if "enhancement_type" not in df.columns:
                    df["enhancement_type"] = enhancement_type

                # Ø³Ø§Ø®Øª unique_key
                df = create_unique_key(df, ["url", "item_name", "last_crawled", "enhancement_type"])

                # ÙÛŒÙ„ØªØ± Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
                new_df = df[~df["unique_key"].isin(existing_keys)]
                existing_keys.update(new_df["unique_key"].tolist())

                if not new_df.empty:
                    # ÙÙ‚Ø· Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
                    new_df = new_df[[col for col in VALID_COLUMNS if col in new_df.columns]]

                    # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ® Ø¨Ù‡ DATE
                    for date_col in ["last_crawled"]:
                        if date_col in new_df.columns:
                            new_df[date_col] = pd.to_datetime(new_df[date_col], errors="coerce").dt.date

                    all_new_records.append(new_df)

    # Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ù‡Ø§ÛŒÛŒ
    if all_new_records:
        final_df = pd.concat(all_new_records, ignore_index=True)
        append_to_bigquery(final_df)
    else:
        print("âš ï¸ No new records found.")

if __name__ == "__main__":
    main()

